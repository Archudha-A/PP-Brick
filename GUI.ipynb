{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e797e928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "[[ 0.19  0.85  0.76  0.09  0.36 14.  ]\n",
      " [ 0.19  0.76  0.76  0.59  0.36  7.  ]\n",
      " [ 0.19  0.85  0.76  0.46  0.36 28.  ]\n",
      " [ 0.19  0.76  0.76  0.23  0.36 14.  ]\n",
      " [ 0.28  0.76  0.66  0.4   0.36  7.  ]\n",
      " [ 0.19  0.76  0.76  0.42  0.36 14.  ]\n",
      " [ 0.28  0.76  0.66  0.66  0.36  7.  ]\n",
      " [ 0.19  0.85  0.76  0.38  0.36 14.  ]\n",
      " [ 0.19  0.76  0.76  0.25  0.36 28.  ]\n",
      " [ 0.28  0.76  0.66  0.23  0.36 28.  ]\n",
      " [ 0.28  0.76  0.66  0.15  0.36  7.  ]\n",
      " [ 0.28  0.76  0.66  0.63  0.36 28.  ]\n",
      " [ 0.28  0.76  0.66  0.42  0.36 14.  ]\n",
      " [ 0.19  0.85  0.76  0.51  0.36 28.  ]\n",
      " [ 0.19  0.76  0.76  0.38  0.36 28.  ]\n",
      " [ 0.28  0.76  0.66  0.36  0.36  7.  ]\n",
      " [ 0.28  0.76  0.66  0.4   0.36 14.  ]\n",
      " [ 0.19  0.76  0.76  0.59  0.36 14.  ]\n",
      " [ 0.19  0.76  0.76  0.36  0.36  7.  ]\n",
      " [ 0.19  0.85  0.76  0.09  0.36 28.  ]\n",
      " [ 0.28  0.76  0.66  0.3   0.36 14.  ]\n",
      " [ 0.28  0.76  0.66  0.59  0.36 28.  ]\n",
      " [ 0.19  0.85  0.76  0.4   0.36 14.  ]\n",
      " [ 0.19  0.85  0.76  0.32  0.36 14.  ]\n",
      " [ 0.28  0.76  0.66  0.19  0.36  7.  ]\n",
      " [ 0.19  0.85  0.76  0.27  0.36  7.  ]\n",
      " [ 0.19  0.85  0.76  0.51  0.36  7.  ]\n",
      " [ 0.28  0.76  0.66  0.46  0.36  7.  ]\n",
      " [ 0.19  0.76  0.76  0.46  0.36  7.  ]\n",
      " [ 0.19  0.85  0.76  0.55  0.36  7.  ]\n",
      " [ 0.19  0.76  0.76  0.53  0.36  7.  ]\n",
      " [ 0.19  0.85  0.76  0.25  0.36 28.  ]\n",
      " [ 0.19  0.76  0.76  0.55  0.36 14.  ]\n",
      " [ 0.28  0.76  0.66  0.36  0.36 14.  ]\n",
      " [ 0.28  0.76  0.66  0.32  0.36 28.  ]\n",
      " [ 0.19  0.85  0.76  0.21  0.36  7.  ]\n",
      " [ 0.19  0.76  0.76  0.09  0.36 14.  ]\n",
      " [ 0.19  0.85  0.76  0.23  0.36 28.  ]\n",
      " [ 0.19  0.76  0.76  0.66  0.36  7.  ]\n",
      " [ 0.19  0.76  0.76  0.63  0.36  7.  ]\n",
      " [ 0.19  0.85  0.76  0.57  0.36  7.  ]\n",
      " [ 0.19  0.76  0.76  0.19  0.36 14.  ]\n",
      " [ 0.28  0.76  0.66  0.17  0.36 28.  ]\n",
      " [ 0.19  0.85  0.76  0.46  0.36  7.  ]\n",
      " [ 0.19  0.85  0.76  0.42  0.36 28.  ]\n",
      " [ 0.28  0.76  0.66  0.21  0.36 14.  ]\n",
      " [ 0.19  0.76  0.76  0.09  0.36  7.  ]\n",
      " [ 0.28  0.76  0.66  0.63  0.36  7.  ]\n",
      " [ 0.19  0.76  0.76  0.15  0.36  7.  ]\n",
      " [ 0.28  0.76  0.66  0.09  0.36  7.  ]\n",
      " [ 0.19  0.85  0.76  0.38  0.36  7.  ]\n",
      " [ 0.19  0.76  0.76  0.13  0.36 28.  ]\n",
      " [ 0.28  0.76  0.66  0.51  0.36 14.  ]\n",
      " [ 0.28  0.76  0.66  0.28  0.36 28.  ]]\n",
      "[[-6.92424765e-01  1.39960856e+00  6.92424765e-01 -1.70727778e+00\n",
      "   4.44089210e-16 -3.08503292e-01]\n",
      " [-6.92424765e-01 -7.14485487e-01  6.92424765e-01  1.32638461e+00\n",
      "   4.44089210e-16 -1.11135523e+00]\n",
      " [-6.92424765e-01  1.39960856e+00  6.92424765e-01  5.37632389e-01\n",
      "   4.44089210e-16  1.29720059e+00]\n",
      " [-6.92424765e-01 -7.14485487e-01  6.92424765e-01 -8.57852308e-01\n",
      "   4.44089210e-16 -3.08503292e-01]\n",
      " [ 1.44420022e+00 -7.14485487e-01 -1.44420022e+00  1.73592903e-01\n",
      "   4.44089210e-16 -1.11135523e+00]\n",
      " [-6.92424765e-01 -7.14485487e-01  6.92424765e-01  2.94939399e-01\n",
      "   4.44089210e-16 -3.08503292e-01]\n",
      " [ 1.44420022e+00 -7.14485487e-01 -1.44420022e+00  1.75109734e+00\n",
      "   4.44089210e-16 -1.11135523e+00]\n",
      " [-6.92424765e-01  1.39960856e+00  6.92424765e-01  5.22464077e-02\n",
      "   4.44089210e-16 -3.08503292e-01]\n",
      " [-6.92424765e-01 -7.14485487e-01  6.92424765e-01 -7.36505812e-01\n",
      "   4.44089210e-16  1.29720059e+00]\n",
      " [ 1.44420022e+00 -7.14485487e-01 -1.44420022e+00 -8.57852308e-01\n",
      "   4.44089210e-16  1.29720059e+00]\n",
      " [ 1.44420022e+00 -7.14485487e-01 -1.44420022e+00 -1.34323829e+00\n",
      "   4.44089210e-16 -1.11135523e+00]\n",
      " [ 1.44420022e+00 -7.14485487e-01 -1.44420022e+00  1.56907760e+00\n",
      "   4.44089210e-16  1.29720059e+00]\n",
      " [ 1.44420022e+00 -7.14485487e-01 -1.44420022e+00  2.94939399e-01\n",
      "   4.44089210e-16 -3.08503292e-01]\n",
      " [-6.92424765e-01  1.39960856e+00  6.92424765e-01  8.40998628e-01\n",
      "   4.44089210e-16  1.29720059e+00]\n",
      " [-6.92424765e-01 -7.14485487e-01  6.92424765e-01  5.22464077e-02\n",
      "   4.44089210e-16  1.29720059e+00]\n",
      " [ 1.44420022e+00 -7.14485487e-01 -1.44420022e+00 -6.91000877e-02\n",
      "   4.44089210e-16 -1.11135523e+00]\n",
      " [ 1.44420022e+00 -7.14485487e-01 -1.44420022e+00  1.73592903e-01\n",
      "   4.44089210e-16 -3.08503292e-01]\n",
      " [-6.92424765e-01 -7.14485487e-01  6.92424765e-01  1.32638461e+00\n",
      "   4.44089210e-16 -3.08503292e-01]\n",
      " [-6.92424765e-01 -7.14485487e-01  6.92424765e-01 -6.91000877e-02\n",
      "   4.44089210e-16 -1.11135523e+00]\n",
      " [-6.92424765e-01  1.39960856e+00  6.92424765e-01 -1.70727778e+00\n",
      "   4.44089210e-16  1.29720059e+00]\n",
      " [ 1.44420022e+00 -7.14485487e-01 -1.44420022e+00 -4.33139574e-01\n",
      "   4.44089210e-16 -3.08503292e-01]\n",
      " [ 1.44420022e+00 -7.14485487e-01 -1.44420022e+00  1.32638461e+00\n",
      "   4.44089210e-16  1.29720059e+00]\n",
      " [-6.92424765e-01  1.39960856e+00  6.92424765e-01  1.73592903e-01\n",
      "   4.44089210e-16 -3.08503292e-01]\n",
      " [-6.92424765e-01  1.39960856e+00  6.92424765e-01 -3.11793079e-01\n",
      "   4.44089210e-16 -3.08503292e-01]\n",
      " [ 1.44420022e+00 -7.14485487e-01 -1.44420022e+00 -1.10054530e+00\n",
      "   4.44089210e-16 -1.11135523e+00]\n",
      " [-6.92424765e-01  1.39960856e+00  6.92424765e-01 -6.15159317e-01\n",
      "   4.44089210e-16 -1.11135523e+00]\n",
      " [-6.92424765e-01  1.39960856e+00  6.92424765e-01  8.40998628e-01\n",
      "   4.44089210e-16 -1.11135523e+00]\n",
      " [ 1.44420022e+00 -7.14485487e-01 -1.44420022e+00  5.37632389e-01\n",
      "   4.44089210e-16 -1.11135523e+00]\n",
      " [-6.92424765e-01 -7.14485487e-01  6.92424765e-01  5.37632389e-01\n",
      "   4.44089210e-16 -1.11135523e+00]\n",
      " [-6.92424765e-01  1.39960856e+00  6.92424765e-01  1.08369162e+00\n",
      "   4.44089210e-16 -1.11135523e+00]\n",
      " [-6.92424765e-01 -7.14485487e-01  6.92424765e-01  9.62345123e-01\n",
      "   4.44089210e-16 -1.11135523e+00]\n",
      " [-6.92424765e-01  1.39960856e+00  6.92424765e-01 -7.36505812e-01\n",
      "   4.44089210e-16  1.29720059e+00]\n",
      " [-6.92424765e-01 -7.14485487e-01  6.92424765e-01  1.08369162e+00\n",
      "   4.44089210e-16 -3.08503292e-01]\n",
      " [ 1.44420022e+00 -7.14485487e-01 -1.44420022e+00 -6.91000877e-02\n",
      "   4.44089210e-16 -3.08503292e-01]\n",
      " [ 1.44420022e+00 -7.14485487e-01 -1.44420022e+00 -3.11793079e-01\n",
      "   4.44089210e-16  1.29720059e+00]\n",
      " [-6.92424765e-01  1.39960856e+00  6.92424765e-01 -9.79198803e-01\n",
      "   4.44089210e-16 -1.11135523e+00]\n",
      " [-6.92424765e-01 -7.14485487e-01  6.92424765e-01 -1.70727778e+00\n",
      "   4.44089210e-16 -3.08503292e-01]\n",
      " [-6.92424765e-01  1.39960856e+00  6.92424765e-01 -8.57852308e-01\n",
      "   4.44089210e-16  1.29720059e+00]\n",
      " [-6.92424765e-01 -7.14485487e-01  6.92424765e-01  1.75109734e+00\n",
      "   4.44089210e-16 -1.11135523e+00]\n",
      " [-6.92424765e-01 -7.14485487e-01  6.92424765e-01  1.56907760e+00\n",
      "   4.44089210e-16 -1.11135523e+00]\n",
      " [-6.92424765e-01  1.39960856e+00  6.92424765e-01  1.20503811e+00\n",
      "   4.44089210e-16 -1.11135523e+00]\n",
      " [-6.92424765e-01 -7.14485487e-01  6.92424765e-01 -1.10054530e+00\n",
      "   4.44089210e-16 -3.08503292e-01]\n",
      " [ 1.44420022e+00 -7.14485487e-01 -1.44420022e+00 -1.22189179e+00\n",
      "   4.44089210e-16  1.29720059e+00]\n",
      " [-6.92424765e-01  1.39960856e+00  6.92424765e-01  5.37632389e-01\n",
      "   4.44089210e-16 -1.11135523e+00]\n",
      " [-6.92424765e-01  1.39960856e+00  6.92424765e-01  2.94939399e-01\n",
      "   4.44089210e-16  1.29720059e+00]\n",
      " [ 1.44420022e+00 -7.14485487e-01 -1.44420022e+00 -9.79198803e-01\n",
      "   4.44089210e-16 -3.08503292e-01]\n",
      " [-6.92424765e-01 -7.14485487e-01  6.92424765e-01 -1.70727778e+00\n",
      "   4.44089210e-16 -1.11135523e+00]\n",
      " [ 1.44420022e+00 -7.14485487e-01 -1.44420022e+00  1.56907760e+00\n",
      "   4.44089210e-16 -1.11135523e+00]\n",
      " [-6.92424765e-01 -7.14485487e-01  6.92424765e-01 -1.34323829e+00\n",
      "   4.44089210e-16 -1.11135523e+00]\n",
      " [ 1.44420022e+00 -7.14485487e-01 -1.44420022e+00 -1.70727778e+00\n",
      "   4.44089210e-16 -1.11135523e+00]\n",
      " [-6.92424765e-01  1.39960856e+00  6.92424765e-01  5.22464077e-02\n",
      "   4.44089210e-16 -1.11135523e+00]\n",
      " [-6.92424765e-01 -7.14485487e-01  6.92424765e-01 -1.46458478e+00\n",
      "   4.44089210e-16  1.29720059e+00]\n",
      " [ 1.44420022e+00 -7.14485487e-01 -1.44420022e+00  8.40998628e-01\n",
      "   4.44089210e-16 -3.08503292e-01]\n",
      " [ 1.44420022e+00 -7.14485487e-01 -1.44420022e+00 -5.54486069e-01\n",
      "   4.44089210e-16  1.29720059e+00]]\n",
      "WARNING:tensorflow:From C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/200\n",
      "WARNING:tensorflow:From C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "8/8 [==============================] - 3s 165ms/step - loss: 134.7455 - mae: 11.1891 - val_loss: 116.2518 - val_mae: 10.3591\n",
      "Epoch 2/200\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 123.8290 - mae: 10.6744 - val_loss: 106.0006 - val_mae: 9.8503\n",
      "Epoch 3/200\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 113.1908 - mae: 10.1597 - val_loss: 95.5565 - val_mae: 9.3019\n",
      "Epoch 4/200\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 102.3382 - mae: 9.6016 - val_loss: 84.6664 - val_mae: 8.6956\n",
      "Epoch 5/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 90.6548 - mae: 8.9724 - val_loss: 73.3588 - val_mae: 8.0185\n",
      "Epoch 6/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 78.5346 - mae: 8.2634 - val_loss: 61.5024 - val_mae: 7.2490\n",
      "Epoch 7/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 65.8561 - mae: 7.4544 - val_loss: 49.5458 - val_mae: 6.3895\n",
      "Epoch 8/200\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 52.9664 - mae: 6.5370 - val_loss: 37.9956 - val_mae: 5.4415\n",
      "Epoch 9/200\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 40.6114 - mae: 5.5683 - val_loss: 27.6634 - val_mae: 4.4634\n",
      "Epoch 10/200\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 29.8185 - mae: 4.5960 - val_loss: 18.9126 - val_mae: 3.5349\n",
      "Epoch 11/200\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 20.2883 - mae: 3.6724 - val_loss: 12.3650 - val_mae: 2.7479\n",
      "Epoch 12/200\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 13.3337 - mae: 2.9021 - val_loss: 7.9668 - val_mae: 2.2476\n",
      "Epoch 13/200\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 8.9141 - mae: 2.3085 - val_loss: 5.2872 - val_mae: 1.8804\n",
      "Epoch 14/200\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 6.1214 - mae: 1.9814 - val_loss: 3.8975 - val_mae: 1.6503\n",
      "Epoch 15/200\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 4.4285 - mae: 1.7280 - val_loss: 3.2017 - val_mae: 1.4982\n",
      "Epoch 16/200\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 3.6982 - mae: 1.5767 - val_loss: 2.7491 - val_mae: 1.3738\n",
      "Epoch 17/200\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 3.1260 - mae: 1.4432 - val_loss: 2.4094 - val_mae: 1.2670\n",
      "Epoch 18/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 2.8237 - mae: 1.3664 - val_loss: 2.1750 - val_mae: 1.1744\n",
      "Epoch 19/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 2.5853 - mae: 1.2930 - val_loss: 1.9884 - val_mae: 1.0955\n",
      "Epoch 20/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 2.4250 - mae: 1.2437 - val_loss: 1.8860 - val_mae: 1.0444\n",
      "Epoch 21/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 2.3294 - mae: 1.2116 - val_loss: 1.8069 - val_mae: 1.0108\n",
      "Epoch 22/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 2.2460 - mae: 1.1868 - val_loss: 1.7577 - val_mae: 0.9962\n",
      "Epoch 23/200\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 2.1840 - mae: 1.1678 - val_loss: 1.7077 - val_mae: 0.9800\n",
      "Epoch 24/200\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 2.1300 - mae: 1.1535 - val_loss: 1.6656 - val_mae: 0.9678\n",
      "Epoch 25/200\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 2.0755 - mae: 1.1378 - val_loss: 1.6314 - val_mae: 0.9582\n",
      "Epoch 26/200\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 2.0280 - mae: 1.1246 - val_loss: 1.5976 - val_mae: 0.9473\n",
      "Epoch 27/200\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1.9853 - mae: 1.1138 - val_loss: 1.5652 - val_mae: 0.9394\n",
      "Epoch 28/200\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1.9419 - mae: 1.1008 - val_loss: 1.5396 - val_mae: 0.9265\n",
      "Epoch 29/200\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 1.9036 - mae: 1.0865 - val_loss: 1.5144 - val_mae: 0.9151\n",
      "Epoch 30/200\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.8632 - mae: 1.0762 - val_loss: 1.4854 - val_mae: 0.9108\n",
      "Epoch 31/200\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.8256 - mae: 1.0660 - val_loss: 1.4597 - val_mae: 0.9039\n",
      "Epoch 32/200\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 1.7958 - mae: 1.0565 - val_loss: 1.4432 - val_mae: 0.8936\n",
      "Epoch 33/200\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 1.7565 - mae: 1.0441 - val_loss: 1.4168 - val_mae: 0.8838\n",
      "Epoch 34/200\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 1.7289 - mae: 1.0357 - val_loss: 1.3957 - val_mae: 0.8722\n",
      "Epoch 35/200\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 1.6903 - mae: 1.0247 - val_loss: 1.3725 - val_mae: 0.8713\n",
      "Epoch 36/200\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 1.6618 - mae: 1.0163 - val_loss: 1.3541 - val_mae: 0.8662\n",
      "Epoch 37/200\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 1.6297 - mae: 1.0062 - val_loss: 1.3344 - val_mae: 0.8587\n",
      "Epoch 38/200\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 1.6047 - mae: 0.9976 - val_loss: 1.3185 - val_mae: 0.8536\n",
      "Epoch 39/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 1.5758 - mae: 0.9883 - val_loss: 1.3012 - val_mae: 0.8484\n",
      "Epoch 40/200\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1.5570 - mae: 0.9853 - val_loss: 1.2777 - val_mae: 0.8420\n",
      "Epoch 41/200\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 1.5295 - mae: 0.9739 - val_loss: 1.2679 - val_mae: 0.8330\n",
      "Epoch 42/200\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.5062 - mae: 0.9665 - val_loss: 1.2492 - val_mae: 0.8268\n",
      "Epoch 43/200\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 1.4781 - mae: 0.9573 - val_loss: 1.2308 - val_mae: 0.8233\n",
      "Epoch 44/200\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 1.4591 - mae: 0.9496 - val_loss: 1.2189 - val_mae: 0.8182\n",
      "Epoch 45/200\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 1.4394 - mae: 0.9456 - val_loss: 1.1976 - val_mae: 0.8189\n",
      "Epoch 46/200\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 1.4186 - mae: 0.9417 - val_loss: 1.1820 - val_mae: 0.8145\n",
      "Epoch 47/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 1.3967 - mae: 0.9329 - val_loss: 1.1689 - val_mae: 0.8038\n",
      "Epoch 48/200\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 1.3763 - mae: 0.9235 - val_loss: 1.1574 - val_mae: 0.7966\n",
      "Epoch 49/200\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 1.3605 - mae: 0.9174 - val_loss: 1.1409 - val_mae: 0.7913\n",
      "Epoch 50/200\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 1.3412 - mae: 0.9105 - val_loss: 1.1343 - val_mae: 0.7855\n",
      "Epoch 51/200\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 1.3280 - mae: 0.9071 - val_loss: 1.1199 - val_mae: 0.7851\n",
      "Epoch 52/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 1.3134 - mae: 0.9033 - val_loss: 1.1103 - val_mae: 0.7783\n",
      "Epoch 53/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 1.3018 - mae: 0.8989 - val_loss: 1.1057 - val_mae: 0.7838\n",
      "Epoch 54/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 1.2824 - mae: 0.8931 - val_loss: 1.0916 - val_mae: 0.7795\n",
      "Epoch 55/200\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 1.2724 - mae: 0.8919 - val_loss: 1.0730 - val_mae: 0.7795\n",
      "Epoch 56/200\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 1.2536 - mae: 0.8876 - val_loss: 1.0635 - val_mae: 0.7718\n",
      "Epoch 57/200\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 1.2395 - mae: 0.8807 - val_loss: 1.0532 - val_mae: 0.7654\n",
      "Epoch 58/200\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 1.2313 - mae: 0.8742 - val_loss: 1.0518 - val_mae: 0.7585\n",
      "Epoch 59/200\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 1.2184 - mae: 0.8688 - val_loss: 1.0401 - val_mae: 0.7620\n",
      "Epoch 60/200\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1.2062 - mae: 0.8702 - val_loss: 1.0239 - val_mae: 0.7554\n",
      "Epoch 61/200\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1.1889 - mae: 0.8607 - val_loss: 1.0200 - val_mae: 0.7507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/200\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 1.1708 - mae: 0.8520 - val_loss: 1.0093 - val_mae: 0.7486\n",
      "Epoch 63/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 1.1626 - mae: 0.8495 - val_loss: 0.9986 - val_mae: 0.7463\n",
      "Epoch 64/200\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 1.1425 - mae: 0.8429 - val_loss: 0.9911 - val_mae: 0.7439\n",
      "Epoch 65/200\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 1.1297 - mae: 0.8383 - val_loss: 0.9800 - val_mae: 0.7427\n",
      "Epoch 66/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 1.1137 - mae: 0.8342 - val_loss: 0.9618 - val_mae: 0.7358\n",
      "Epoch 67/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 1.1019 - mae: 0.8284 - val_loss: 0.9514 - val_mae: 0.7243\n",
      "Epoch 68/200\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 1.0796 - mae: 0.8148 - val_loss: 0.9378 - val_mae: 0.7180\n",
      "Epoch 69/200\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 1.0642 - mae: 0.8093 - val_loss: 0.9260 - val_mae: 0.7197\n",
      "Epoch 70/200\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 1.0425 - mae: 0.8034 - val_loss: 0.9085 - val_mae: 0.7127\n",
      "Epoch 71/200\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 1.0250 - mae: 0.7986 - val_loss: 0.8940 - val_mae: 0.7064\n",
      "Epoch 72/200\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 1.0071 - mae: 0.7851 - val_loss: 0.8834 - val_mae: 0.6966\n",
      "Epoch 73/200\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.9837 - mae: 0.7719 - val_loss: 0.8654 - val_mae: 0.6869\n",
      "Epoch 74/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.9665 - mae: 0.7668 - val_loss: 0.8489 - val_mae: 0.6805\n",
      "Epoch 75/200\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.9422 - mae: 0.7575 - val_loss: 0.8323 - val_mae: 0.6758\n",
      "Epoch 76/200\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.9217 - mae: 0.7479 - val_loss: 0.8188 - val_mae: 0.6696\n",
      "Epoch 77/200\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.9034 - mae: 0.7359 - val_loss: 0.8068 - val_mae: 0.6595\n",
      "Epoch 78/200\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.8856 - mae: 0.7269 - val_loss: 0.7866 - val_mae: 0.6554\n",
      "Epoch 79/200\n",
      "8/8 [==============================] - 1s 89ms/step - loss: 0.8588 - mae: 0.7164 - val_loss: 0.7715 - val_mae: 0.6471\n",
      "Epoch 80/200\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 0.8395 - mae: 0.7061 - val_loss: 0.7531 - val_mae: 0.6364\n",
      "Epoch 81/200\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.8141 - mae: 0.6937 - val_loss: 0.7338 - val_mae: 0.6267\n",
      "Epoch 82/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.7948 - mae: 0.6835 - val_loss: 0.7221 - val_mae: 0.6191\n",
      "Epoch 83/200\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.7734 - mae: 0.6712 - val_loss: 0.7036 - val_mae: 0.6126\n",
      "Epoch 84/200\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.7567 - mae: 0.6628 - val_loss: 0.6826 - val_mae: 0.6031\n",
      "Epoch 85/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.7344 - mae: 0.6518 - val_loss: 0.6654 - val_mae: 0.5934\n",
      "Epoch 86/200\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.7149 - mae: 0.6426 - val_loss: 0.6495 - val_mae: 0.5877\n",
      "Epoch 87/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.6954 - mae: 0.6289 - val_loss: 0.6385 - val_mae: 0.5789\n",
      "Epoch 88/200\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.6733 - mae: 0.6158 - val_loss: 0.6189 - val_mae: 0.5695\n",
      "Epoch 89/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.6515 - mae: 0.6066 - val_loss: 0.6061 - val_mae: 0.5632\n",
      "Epoch 90/200\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.6332 - mae: 0.5968 - val_loss: 0.5900 - val_mae: 0.5563\n",
      "Epoch 91/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.6112 - mae: 0.5853 - val_loss: 0.5694 - val_mae: 0.5464\n",
      "Epoch 92/200\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.5933 - mae: 0.5756 - val_loss: 0.5552 - val_mae: 0.5403\n",
      "Epoch 93/200\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.5774 - mae: 0.5686 - val_loss: 0.5414 - val_mae: 0.5328\n",
      "Epoch 94/200\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.5597 - mae: 0.5591 - val_loss: 0.5238 - val_mae: 0.5250\n",
      "Epoch 95/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.5426 - mae: 0.5489 - val_loss: 0.5109 - val_mae: 0.5202\n",
      "Epoch 96/200\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.5273 - mae: 0.5403 - val_loss: 0.5074 - val_mae: 0.5217\n",
      "Epoch 97/200\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.5079 - mae: 0.5285 - val_loss: 0.4907 - val_mae: 0.5106\n",
      "Epoch 98/200\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.4893 - mae: 0.5167 - val_loss: 0.4755 - val_mae: 0.5043\n",
      "Epoch 99/200\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.4738 - mae: 0.5057 - val_loss: 0.4638 - val_mae: 0.5007\n",
      "Epoch 100/200\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.4550 - mae: 0.4945 - val_loss: 0.4533 - val_mae: 0.4964\n",
      "Epoch 101/200\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.4424 - mae: 0.4859 - val_loss: 0.4463 - val_mae: 0.4947\n",
      "Epoch 102/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.4314 - mae: 0.4801 - val_loss: 0.4308 - val_mae: 0.4828\n",
      "Epoch 103/200\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.4113 - mae: 0.4670 - val_loss: 0.4248 - val_mae: 0.4802\n",
      "Epoch 104/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.4033 - mae: 0.4636 - val_loss: 0.4172 - val_mae: 0.4777\n",
      "Epoch 105/200\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.3891 - mae: 0.4522 - val_loss: 0.4040 - val_mae: 0.4649\n",
      "Epoch 106/200\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.3802 - mae: 0.4467 - val_loss: 0.3954 - val_mae: 0.4592\n",
      "Epoch 107/200\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.3706 - mae: 0.4400 - val_loss: 0.3967 - val_mae: 0.4629\n",
      "Epoch 108/200\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.3583 - mae: 0.4321 - val_loss: 0.3821 - val_mae: 0.4568\n",
      "Epoch 109/200\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.3460 - mae: 0.4234 - val_loss: 0.3710 - val_mae: 0.4415\n",
      "Epoch 110/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.3379 - mae: 0.4127 - val_loss: 0.3669 - val_mae: 0.4402\n",
      "Epoch 111/200\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.3290 - mae: 0.4085 - val_loss: 0.3596 - val_mae: 0.4394\n",
      "Epoch 112/200\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.3181 - mae: 0.3989 - val_loss: 0.3510 - val_mae: 0.4256\n",
      "Epoch 113/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.3103 - mae: 0.3923 - val_loss: 0.3430 - val_mae: 0.4268\n",
      "Epoch 114/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.3026 - mae: 0.3890 - val_loss: 0.3367 - val_mae: 0.4200\n",
      "Epoch 115/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.2959 - mae: 0.3829 - val_loss: 0.3370 - val_mae: 0.4202\n",
      "Epoch 116/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.2870 - mae: 0.3746 - val_loss: 0.3253 - val_mae: 0.4127\n",
      "Epoch 117/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.2812 - mae: 0.3663 - val_loss: 0.3217 - val_mae: 0.4046\n",
      "Epoch 118/200\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.2734 - mae: 0.3596 - val_loss: 0.3163 - val_mae: 0.4049\n",
      "Epoch 119/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.2660 - mae: 0.3543 - val_loss: 0.3089 - val_mae: 0.4000\n",
      "Epoch 120/200\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.2606 - mae: 0.3492 - val_loss: 0.3039 - val_mae: 0.3962\n",
      "Epoch 121/200\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.2571 - mae: 0.3440 - val_loss: 0.2942 - val_mae: 0.3833\n",
      "Epoch 122/200\n",
      "8/8 [==============================] - 0s 29ms/step - loss: 0.2506 - mae: 0.3397 - val_loss: 0.2946 - val_mae: 0.3905\n",
      "Epoch 123/200\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.2446 - mae: 0.3321 - val_loss: 0.2867 - val_mae: 0.3756\n",
      "Epoch 124/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 48ms/step - loss: 0.2414 - mae: 0.3278 - val_loss: 0.2860 - val_mae: 0.3808\n",
      "Epoch 125/200\n",
      "8/8 [==============================] - 0s 27ms/step - loss: 0.2361 - mae: 0.3219 - val_loss: 0.2784 - val_mae: 0.3659\n",
      "Epoch 126/200\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.2306 - mae: 0.3162 - val_loss: 0.2735 - val_mae: 0.3691\n",
      "Epoch 127/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.2276 - mae: 0.3140 - val_loss: 0.2762 - val_mae: 0.3651\n",
      "Epoch 128/200\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.2211 - mae: 0.3026 - val_loss: 0.2642 - val_mae: 0.3528\n",
      "Epoch 129/200\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.2175 - mae: 0.2999 - val_loss: 0.2620 - val_mae: 0.3516\n",
      "Epoch 130/200\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.2145 - mae: 0.2975 - val_loss: 0.2557 - val_mae: 0.3470\n",
      "Epoch 131/200\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.2098 - mae: 0.2926 - val_loss: 0.2536 - val_mae: 0.3449\n",
      "Epoch 132/200\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.2067 - mae: 0.2886 - val_loss: 0.2486 - val_mae: 0.3391\n",
      "Epoch 133/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.2030 - mae: 0.2823 - val_loss: 0.2442 - val_mae: 0.3278\n",
      "Epoch 134/200\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.2020 - mae: 0.2801 - val_loss: 0.2430 - val_mae: 0.3309\n",
      "Epoch 135/200\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1967 - mae: 0.2741 - val_loss: 0.2359 - val_mae: 0.3247\n",
      "Epoch 136/200\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1927 - mae: 0.2678 - val_loss: 0.2328 - val_mae: 0.3137\n",
      "Epoch 137/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1909 - mae: 0.2638 - val_loss: 0.2305 - val_mae: 0.3119\n",
      "Epoch 138/200\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1882 - mae: 0.2636 - val_loss: 0.2269 - val_mae: 0.3161\n",
      "Epoch 139/200\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1840 - mae: 0.2584 - val_loss: 0.2219 - val_mae: 0.3045\n",
      "Epoch 140/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1805 - mae: 0.2497 - val_loss: 0.2190 - val_mae: 0.2982\n",
      "Epoch 141/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1785 - mae: 0.2478 - val_loss: 0.2155 - val_mae: 0.2983\n",
      "Epoch 142/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1764 - mae: 0.2459 - val_loss: 0.2142 - val_mae: 0.2938\n",
      "Epoch 143/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1740 - mae: 0.2417 - val_loss: 0.2072 - val_mae: 0.2879\n",
      "Epoch 144/200\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1709 - mae: 0.2373 - val_loss: 0.2076 - val_mae: 0.2837\n",
      "Epoch 145/200\n",
      "8/8 [==============================] - 0s 17ms/step - loss: 0.1678 - mae: 0.2330 - val_loss: 0.2038 - val_mae: 0.2825\n",
      "Epoch 146/200\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1660 - mae: 0.2317 - val_loss: 0.1996 - val_mae: 0.2771\n",
      "Epoch 147/200\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1637 - mae: 0.2255 - val_loss: 0.1945 - val_mae: 0.2655\n",
      "Epoch 148/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1610 - mae: 0.2206 - val_loss: 0.1947 - val_mae: 0.2672\n",
      "Epoch 149/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1595 - mae: 0.2187 - val_loss: 0.1930 - val_mae: 0.2643\n",
      "Epoch 150/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1556 - mae: 0.2142 - val_loss: 0.1884 - val_mae: 0.2631\n",
      "Epoch 151/200\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1550 - mae: 0.2131 - val_loss: 0.1861 - val_mae: 0.2573\n",
      "Epoch 152/200\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1527 - mae: 0.2072 - val_loss: 0.1822 - val_mae: 0.2469\n",
      "Epoch 153/200\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.1502 - mae: 0.2027 - val_loss: 0.1802 - val_mae: 0.2457\n",
      "Epoch 154/200\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.1477 - mae: 0.1994 - val_loss: 0.1771 - val_mae: 0.2437\n",
      "Epoch 155/200\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.1461 - mae: 0.1948 - val_loss: 0.1755 - val_mae: 0.2379\n",
      "Epoch 156/200\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1462 - mae: 0.1944 - val_loss: 0.1749 - val_mae: 0.2356\n",
      "Epoch 157/200\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 0.1423 - mae: 0.1894 - val_loss: 0.1709 - val_mae: 0.2340\n",
      "Epoch 158/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1410 - mae: 0.1869 - val_loss: 0.1690 - val_mae: 0.2281\n",
      "Epoch 159/200\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.1393 - mae: 0.1828 - val_loss: 0.1655 - val_mae: 0.2222\n",
      "Epoch 160/200\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1377 - mae: 0.1787 - val_loss: 0.1639 - val_mae: 0.2198\n",
      "Epoch 161/200\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1363 - mae: 0.1771 - val_loss: 0.1642 - val_mae: 0.2165\n",
      "Epoch 162/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1349 - mae: 0.1732 - val_loss: 0.1614 - val_mae: 0.2149\n",
      "Epoch 163/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1328 - mae: 0.1708 - val_loss: 0.1599 - val_mae: 0.2138\n",
      "Epoch 164/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1313 - mae: 0.1683 - val_loss: 0.1577 - val_mae: 0.2093\n",
      "Epoch 165/200\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1296 - mae: 0.1639 - val_loss: 0.1570 - val_mae: 0.2045\n",
      "Epoch 166/200\n",
      "8/8 [==============================] - 0s 28ms/step - loss: 0.1286 - mae: 0.1601 - val_loss: 0.1531 - val_mae: 0.2016\n",
      "Epoch 167/200\n",
      "8/8 [==============================] - 0s 15ms/step - loss: 0.1277 - mae: 0.1588 - val_loss: 0.1532 - val_mae: 0.1986\n",
      "Epoch 168/200\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 0.1257 - mae: 0.1541 - val_loss: 0.1506 - val_mae: 0.1953\n",
      "Epoch 169/200\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1250 - mae: 0.1539 - val_loss: 0.1495 - val_mae: 0.1941\n",
      "Epoch 170/200\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1236 - mae: 0.1515 - val_loss: 0.1481 - val_mae: 0.1901\n",
      "Epoch 171/200\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1223 - mae: 0.1481 - val_loss: 0.1476 - val_mae: 0.1872\n",
      "Epoch 172/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1211 - mae: 0.1452 - val_loss: 0.1444 - val_mae: 0.1839\n",
      "Epoch 173/200\n",
      "8/8 [==============================] - 0s 12ms/step - loss: 0.1204 - mae: 0.1439 - val_loss: 0.1422 - val_mae: 0.1810\n",
      "Epoch 174/200\n",
      "8/8 [==============================] - 0s 16ms/step - loss: 0.1204 - mae: 0.1428 - val_loss: 0.1437 - val_mae: 0.1791\n",
      "Epoch 175/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1186 - mae: 0.1390 - val_loss: 0.1401 - val_mae: 0.1734\n",
      "Epoch 176/200\n",
      "8/8 [==============================] - 0s 13ms/step - loss: 0.1178 - mae: 0.1370 - val_loss: 0.1382 - val_mae: 0.1710\n",
      "Epoch 177/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1164 - mae: 0.1341 - val_loss: 0.1417 - val_mae: 0.1749\n",
      "Epoch 178/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1159 - mae: 0.1349 - val_loss: 0.1385 - val_mae: 0.1716\n",
      "Epoch 179/200\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.1151 - mae: 0.1326 - val_loss: 0.1374 - val_mae: 0.1693\n",
      "Epoch 180/200\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1142 - mae: 0.1304 - val_loss: 0.1356 - val_mae: 0.1656\n",
      "Epoch 181/200\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1128 - mae: 0.1272 - val_loss: 0.1347 - val_mae: 0.1635\n",
      "Epoch 182/200\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1120 - mae: 0.1249 - val_loss: 0.1343 - val_mae: 0.1622\n",
      "Epoch 183/200\n",
      "8/8 [==============================] - 0s 14ms/step - loss: 0.1118 - mae: 0.1255 - val_loss: 0.1346 - val_mae: 0.1618\n",
      "Epoch 184/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1105 - mae: 0.1238 - val_loss: 0.1316 - val_mae: 0.1581\n",
      "Epoch 185/200\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1102 - mae: 0.1225 - val_loss: 0.1308 - val_mae: 0.1555\n",
      "Epoch 186/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1094 - mae: 0.1198 - val_loss: 0.1299 - val_mae: 0.1536\n",
      "Epoch 187/200\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1088 - mae: 0.1188 - val_loss: 0.1300 - val_mae: 0.1544\n",
      "Epoch 188/200\n",
      "8/8 [==============================] - 0s 11ms/step - loss: 0.1078 - mae: 0.1167 - val_loss: 0.1280 - val_mae: 0.1517\n",
      "Epoch 189/200\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1073 - mae: 0.1165 - val_loss: 0.1261 - val_mae: 0.1475\n",
      "Epoch 190/200\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1070 - mae: 0.1153 - val_loss: 0.1256 - val_mae: 0.1459\n",
      "Epoch 191/200\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1062 - mae: 0.1133 - val_loss: 0.1259 - val_mae: 0.1483\n",
      "Epoch 192/200\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1053 - mae: 0.1115 - val_loss: 0.1243 - val_mae: 0.1445\n",
      "Epoch 193/200\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1061 - mae: 0.1136 - val_loss: 0.1212 - val_mae: 0.1402\n",
      "Epoch 194/200\n",
      "8/8 [==============================] - 0s 8ms/step - loss: 0.1037 - mae: 0.1082 - val_loss: 0.1256 - val_mae: 0.1488\n",
      "Epoch 195/200\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1045 - mae: 0.1097 - val_loss: 0.1217 - val_mae: 0.1415\n",
      "Epoch 196/200\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1034 - mae: 0.1074 - val_loss: 0.1202 - val_mae: 0.1377\n",
      "Epoch 197/200\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1030 - mae: 0.1055 - val_loss: 0.1205 - val_mae: 0.1402\n",
      "Epoch 198/200\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1025 - mae: 0.1055 - val_loss: 0.1192 - val_mae: 0.1380\n",
      "Epoch 199/200\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.1013 - mae: 0.1029 - val_loss: 0.1162 - val_mae: 0.1305\n",
      "Epoch 200/200\n",
      "8/8 [==============================] - 0s 10ms/step - loss: 0.1008 - mae: 0.1023 - val_loss: 0.1166 - val_mae: 0.1341\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from PIL import Image, ImageTk \n",
    "from tkinter import messagebox\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load your dataset (replace 'your_data.csv' with the actual file)\n",
    "df = pd.read_csv('cs.csv')\n",
    "\n",
    "# Features and target\n",
    "X = df[['Cement', 'FlyAsh', 'M Sand ', 'PP waste', 'W/B ratio', 'Age']].values\n",
    "y = df['Compression Strength'].values  # Comprehensive strength\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(X_test)\n",
    "# Normalize the features (important for neural networks)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "print(X_test)\n",
    "# Build the neural network model\n",
    "model = Sequential()\n",
    "\n",
    "# Input layer and first hidden layer with L2 regularization and ReLU activation\n",
    "model.add(Dense(64, input_dim=X_train.shape[1], activation='relu', kernel_regularizer=l2(0.001), bias_regularizer=l2(0.001)))\n",
    "\n",
    "\n",
    "# Second hidden layer\n",
    "model.add(Dense(32, activation='relu', kernel_regularizer=l2(0.001), bias_regularizer=l2(0.001)))\n",
    "\n",
    "# Output layer (since this is a regression problem, no activation function in the output layer)\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, validation_split=0.2, epochs=200, batch_size=22, verbose=1)\n",
    "#y_pred = model.predict(X_test)\n",
    "\n",
    "# Function to predict comprehensive strength using the ML model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c2dbdcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_16476\\1919665718.py:39: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n",
      "  bg_image = bg_image.resize((root.winfo_screenwidth(), root.winfo_screenheight()), Image.ANTIALIAS)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-6.92424765e-01  1.39960856e+00  6.92424765e-01 -1.58593128e+00\n",
      "  4.44089210e-16 -1.11135523e+00]\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "[[7.526775]]\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "Compressive strength:7.53\n"
     ]
    }
   ],
   "source": [
    "def predict_strength():\n",
    "    try:\n",
    "        # Get the input values from the text boxes\n",
    "        cement = float(entry_cement.get())\n",
    "        flyash = float(entry_flyash.get())\n",
    "        MSand= float(entry_msand.get())\n",
    "        pp = float(entry_pp.get())\n",
    "        WBRatio=float(entry_wb.get())\n",
    "        Age=float(entry_age.get())\n",
    "\n",
    "        # Prepare input for the model (as an array)\n",
    "        input_features = np.array([[cement, flyash, MSand, pp, WBRatio, Age]])\n",
    "        input_features=scaler.transform(input_features)\n",
    "        print(input_features[0])\n",
    "        print(model.predict(input_features))\n",
    "\n",
    "        # Use the model to predict comprehensive strength\n",
    "        comprehensive_strength = model.predict(input_features)  # Get the first (and only) prediction\n",
    "        print(f\"Compressive strength:{comprehensive_strength[0][0]:.2f}\")\n",
    "\n",
    "        # Display the result in the result text box\n",
    "        entry_result.delete(0, tk.END)  # Clear previous result\n",
    "        entry_result.insert(0, f\"{comprehensive_strength[0][0]:.2f}\")\n",
    "\n",
    "    except ValueError:\n",
    "        messagebox.showerror(\"Input Error\", \"Please enter valid numbers for all inputs.\")\n",
    "root = tk.Tk()\n",
    "root.title(\"Compressive Strength Predictor\")\n",
    "root.attributes('-fullscreen', True)  # Full screen mode\n",
    "root.configure(bg=\"#f0f8ff\")  # Light blue background\n",
    "\n",
    "# Exit full-screen on pressing \"Esc\"\n",
    "def exit_fullscreen(event=None):\n",
    "    root.attributes('-fullscreen', False)\n",
    "\n",
    "root.bind(\"<Escape>\", exit_fullscreen)\n",
    "# Load the background image\n",
    "bg_image = Image.open(\"img1.jpeg\")\n",
    "bg_image = bg_image.resize((root.winfo_screenwidth(), root.winfo_screenheight()), Image.ANTIALIAS)\n",
    "bg_photo = ImageTk.PhotoImage(bg_image)\n",
    "\n",
    "# Create a label for the background image\n",
    "bg_label = tk.Label(root, image=bg_photo)\n",
    "bg_label.place(x=0, y=0, relwidth=1, relheight=1)  # Full screen background\n",
    "\n",
    "# Customize the font styles\n",
    "label_font = (\"Helvetica\", 16, \"bold\")\n",
    "entry_font = (\"Helvetica\", 16)\n",
    "button_font = (\"Helvetica\", 16, \"bold\")\n",
    "\n",
    "# Load images (replace 'path_to_image' with actual file path)\n",
    "\n",
    "\n",
    "# Create and position labels and text entry boxes with improved styling\n",
    "tk.Label(root, text=\"Cement:\", font=label_font, bg=\"white\").grid(row=1, column=0, padx=10, pady=2,sticky=\"e\")\n",
    "entry_cement = tk.Entry(root, font=entry_font, width=20)\n",
    "entry_cement.grid(row=1, column=1, padx=10, pady=10)\n",
    "\n",
    "tk.Label(root, text=\"Flyash:\", font=label_font, bg=\"white\").grid(row=2, column=0, padx=10, pady=10,sticky=\"e\")\n",
    "entry_flyash = tk.Entry(root, font=entry_font, width=20)\n",
    "entry_flyash.grid(row=2, column=1, padx=10, pady=10)\n",
    "\n",
    "tk.Label(root, text=\"M Sand:\", font=label_font, bg=\"white\").grid(row=3, column=0, padx=10, pady=10,sticky=\"e\")\n",
    "entry_msand = tk.Entry(root, font=entry_font, width=20)\n",
    "entry_msand.grid(row=3, column=1, padx=10, pady=10)\n",
    "\n",
    "tk.Label(root, text=\"PP Waste:\", font=label_font, bg=\"white\").grid(row=4, column=0, padx=10, pady=10,sticky=\"e\")\n",
    "entry_pp = tk.Entry(root, font=entry_font, width=20)\n",
    "entry_pp.grid(row=4, column=1, padx=10, pady=10)\n",
    "\n",
    "tk.Label(root, text=\"W/B Ratio:\", font=label_font, bg=\"white\").grid(row=5, column=0, padx=10, pady=10,sticky=\"e\")\n",
    "entry_wb = tk.Entry(root, font=entry_font, width=20)\n",
    "entry_wb.grid(row=5, column=1, padx=10, pady=10)\n",
    "\n",
    "tk.Label(root, text=\"Age:\", font=label_font, bg=\"white\").grid(row=6, column=0, padx=10, pady=10,sticky=\"e\")\n",
    "entry_age = tk.Entry(root, font=entry_font, width=20)\n",
    "entry_age.grid(row=6, column=1, padx=10, pady=10)\n",
    "\n",
    "tk.Label(root, text=\"Predicted Comprehensive Strength:\", font=label_font).grid(row=7, column=0, padx=10, pady=10,sticky=\"e\")\n",
    "entry_result = tk.Entry(root, font=entry_font, width=20)\n",
    "entry_result.grid(row=7, column=1, padx=10, pady=10)\n",
    "\n",
    "\n",
    "\n",
    "# Create a styled button with hover effect\n",
    "def on_enter(e):\n",
    "    predict_button['background'] = '#3cb371'  # Darker green on hover\n",
    "\n",
    "def on_leave(e):\n",
    "    predict_button['background'] = '#32cd32'  # Lighter green normally\n",
    "\n",
    "predict_button = tk.Button(root, text=\"Predict\", font=button_font, bg=\"#32cd32\", fg=\"white\", activebackground=\"#3cb371\", width=20, command=predict_strength)\n",
    "predict_button.grid(row=8, column=0, columnspan=2, pady=20)\n",
    "\n",
    "# Add hover effects\n",
    "predict_button.bind(\"<Enter>\", on_enter)\n",
    "predict_button.bind(\"<Leave>\", on_leave)\n",
    "\n",
    "# Center the entire grid layout\n",
    "root.grid_columnconfigure(0, weight=1)\n",
    "root.grid_columnconfigure(1, weight=1)\n",
    "root.grid_columnconfigure(2, weight=1)\n",
    "root.grid_rowconfigure(0, weight=1)\n",
    "root.grid_rowconfigure(9, weight=1)\n",
    "\n",
    "# Start the GUI main loop (necessary to display the window)\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c490bd83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
